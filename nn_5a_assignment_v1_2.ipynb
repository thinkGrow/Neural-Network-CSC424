{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thinkGrow/Neural-Network-CSC424/blob/main/nn_5a_assignment_v1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFJakpubef1x",
        "outputId": "3746ef60-184a-4120-ac5f-19f3976c3c7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npcs-QsXXkpX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "%matplotlib inline\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "import torch.optim as optim\n",
        "from sklearn.utils import shuffle\n",
        "import torch.utils.data as data_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbu3tpLndOHP",
        "outputId": "2debb76f-2978-42d6-bdf1-311dcb80f625"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "(2,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ],
      "source": [
        "tranform_train = transforms.Compose([transforms.Resize((224,224)), transforms.RandomHorizontalFlip(p=0.7), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "tranform_test = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "#preparing the train, validation and test dataset\n",
        "torch.manual_seed(43)\n",
        "train_ds = CIFAR10(\"data/\", train=True, download=True, transform=tranform_train) #40,000 original images + transforms\n",
        "\n",
        "\n",
        "indices = torch.arange(500)\n",
        "train_ds = data_utils.Subset(train_ds, indices)\n",
        "\n",
        "\n",
        "val_size = 100 #there are 10,000 test images and since there are no transforms performed on the test, we keep the validation as 10,000\n",
        "train_size = len(train_ds) - val_size\n",
        "train_ds, val_ds = random_split(train_ds, [train_size, val_size]) #Extracting the 10,000 validation images from the train set\n",
        "\n",
        "\n",
        "test_ds = CIFAR10(\"data/\", train=False, download=True, transform=tranform_test) #10,000 images\n",
        "indicess = torch.arange(100)\n",
        "test_ds = data_utils.Subset(test_ds, indicess)\n",
        "\n",
        "\n",
        " \n",
        "#passing the train, val and test datasets to the dataloader\n",
        "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
        "test_dl = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "for idx, (data,targets) in enumerate(test_dl):\n",
        "    x_train = (data,targets)\n",
        "x_train = np.array(x_train)\n",
        "print(x_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1wGBQF5d3p3"
      },
      "outputs": [],
      "source": [
        "#inherit from a pytorch class called module\n",
        "class VGG16(nn.Module): \n",
        "    def __init__(self):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1) #kernel = filter\n",
        "        self.conv1_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
        "\n",
        "        self.conv2_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.conv2_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
        "\n",
        "        self.conv3_1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.conv3_3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.conv4_1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.conv4_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.conv4_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "\n",
        "        self.conv5_1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.conv5_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.conv5_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2) #stride is the number of pixel shifts\n",
        "\n",
        "        self.fc1 = nn.Linear(25088, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, 10)\n",
        "\n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1_1(x))  \n",
        "        x = F.relu(self.conv1_2(x))\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = F.relu(self.conv2_1(x))\n",
        "        x = F.relu(self.conv2_2(x))\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = F.relu(self.conv3_1(x))\n",
        "        x = F.relu(self.conv3_2(x))\n",
        "        x = F.relu(self.conv3_3(x))\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = F.relu(self.conv4_1(x))\n",
        "        x = F.relu(self.conv4_2(x))\n",
        "        x = F.relu(self.conv4_3(x))\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = F.relu(self.conv5_1(x))\n",
        "        x = F.relu(self.conv5_2(x))\n",
        "        x = F.relu(self.conv5_3(x))\n",
        "        x = self.maxpool(x)\n",
        "        \n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, 0.5) #dropout was included to combat overfitting\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.dropout(x, 0.5)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bG9z-VoBd8cI"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #training with either cpu or cuda\n",
        "\n",
        "model = VGG16() #to compile the model. define model = vgg16()\n",
        "model = model.to(device=device) #to send the model for training on either cuda or cpu\n",
        "\n",
        "## Loss and optimizer\n",
        "learning_rate = 1e-4 #popular value for lr\n",
        "load_model = True\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr= learning_rate) #Adam seems to be the most popular for deep learning\n",
        "# optimizer = optim.Adam(model.parameters(), lr= learning_rate) #Adam seems to be the most popular for deep learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d26ovGCteHA3",
        "outputId": "bb63b9d3-f6cd-4dfd-e96a-83cd5203ac6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss in epoch 0 :::: 2.3019095148359026\n",
            "Got 8 / 100 with accuracy 8.00\n",
            "Loss in epoch 1 :::: 2.2983737332480296\n",
            "Got 13 / 100 with accuracy 13.00\n",
            "Loss in epoch 2 :::: 2.294271605355399\n",
            "Got 18 / 100 with accuracy 18.00\n",
            "Loss in epoch 3 :::: 2.3000403472355435\n",
            "Got 17 / 100 with accuracy 17.00\n",
            "Loss in epoch 4 :::: 2.295475653239659\n",
            "Got 7 / 100 with accuracy 7.00\n",
            "Loss in epoch 5 :::: 2.2942955834524974\n",
            "Got 8 / 100 with accuracy 8.00\n",
            "Loss in epoch 6 :::: 2.2974556514195035\n",
            "Got 8 / 100 with accuracy 8.00\n",
            "Loss in epoch 7 :::: 2.2908034324645996\n",
            "Got 7 / 100 with accuracy 7.00\n",
            "Loss in epoch 8 :::: 2.2675510815211704\n",
            "Got 14 / 100 with accuracy 14.00\n",
            "Loss in epoch 9 :::: 2.1959242480141774\n",
            "Got 19 / 100 with accuracy 19.00\n",
            "Loss in epoch 10 :::: 2.089652453150068\n",
            "Got 14 / 100 with accuracy 14.00\n",
            "Loss in epoch 11 :::: 2.146241767065866\n",
            "Got 15 / 100 with accuracy 15.00\n",
            "Loss in epoch 12 :::: 2.0633921112333025\n",
            "Got 21 / 100 with accuracy 21.00\n",
            "Loss in epoch 13 :::: 1.9748114517756872\n",
            "Got 18 / 100 with accuracy 18.00\n",
            "Loss in epoch 14 :::: 1.94366455078125\n",
            "Got 21 / 100 with accuracy 21.00\n",
            "Loss in epoch 15 :::: 1.9227026871272497\n",
            "Got 27 / 100 with accuracy 27.00\n",
            "Loss in epoch 16 :::: 1.8871486016682215\n",
            "Got 22 / 100 with accuracy 22.00\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(20): #train the model for 50 epochs\n",
        "    loss_ep = 0\n",
        "    \n",
        "    for batch_idx, (data, targets) in enumerate(train_dl):\n",
        "        data = data.to(device=device) #get gpu support\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        ## Forward Pass\n",
        "        optimizer.zero_grad() \n",
        "        scores = model(data)\n",
        "        loss = criterion(scores,targets)\n",
        "        loss.backward()\n",
        "        optimizer.step() #gradient descent\n",
        "        loss_ep += loss.item()\n",
        "    print(f\"Loss in epoch {epoch} :::: {loss_ep/len(train_dl)}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        num_correct = 0\n",
        "        num_samples = 0\n",
        "        for batch_idx, (data,targets) in enumerate(val_dl):\n",
        "            data = data.to(device=device)\n",
        "            targets = targets.to(device=device)\n",
        "            ## Forward Pass\n",
        "            scores = model(data)\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == targets).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "        print(\n",
        "            f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct) / float(num_samples) * 100:.2f}\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBIMSSBMeKzX"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"vgg16_cifar.pt\") #SAVES THE TRAINED MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAy57186eRva"
      },
      "outputs": [],
      "source": [
        "model = VGG16()\n",
        "model.load_state_dict(torch.load(\"/content/vgg16_cifar.pt\")) #loads the trained model\n",
        "model.eval() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OnowALlDeSKD",
        "outputId": "4a31f9c9-4dd3-427c-ca27-954b042a35c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Got 35 / 100 with accuracy 35.00\n"
          ]
        }
      ],
      "source": [
        "num_correct = 0\n",
        "num_samples = 0\n",
        "for batch_idx, (data,targets) in enumerate(test_dl):\n",
        "    data = data.to(device=\"cpu\")\n",
        "    targets = targets.to(device=\"cpu\")\n",
        "    ## Forward Pass\n",
        "    scores = model(data)\n",
        "    _, predictions = scores.max(1)\n",
        "    num_correct += (predictions == targets).sum()\n",
        "    num_samples += predictions.size(0)\n",
        "print(\n",
        "    f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct) / float(num_samples) * 100:.2f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Elmx1c6FP1pU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "nn_5a_assignment_v1.2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMn/Le8tRld4qyH6crMHGCd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
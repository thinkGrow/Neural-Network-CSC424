{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "Application.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7nGmu9p0eQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5yJ8nS9uqXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EtQnb_sxzwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7fFtnmsuqXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj9bZUdruqXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init_token_idx = tokenizer.cls_token_id\n",
        "eos_token_idx = tokenizer.sep_token_id\n",
        "pad_token_idx = tokenizer.pad_token_id\n",
        "unk_token_idx = tokenizer.unk_token_id\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExdPlypHuqX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_input_length = 400\n",
        "\n",
        "print(max_input_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKxMQPPSuqX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_and_cut(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence) \n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmO_HoSIuqX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext import data\n",
        "\n",
        "TEXT = data.Field(sequential= True,\n",
        "                  batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = tokenize_and_cut,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = init_token_idx,\n",
        "                  eos_token = eos_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "LABEL = data.LabelField(sequential= False,dtype = torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODjHAIrI6yEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext import datasets\n",
        "\n",
        "train_data= data.TabularDataset(\n",
        "    path='/content/drive/My Drive/Research_Shanto/Datasets/Ashik Bhai_Sentiment/Code/real_imb_senti_train_dup.csv',  format='CSV',\n",
        "    fields=[('text', TEXT), ('labels', LABEL)])\n",
        "\n",
        "\n",
        "test_data= data.TabularDataset(\n",
        "    path='/content/drive/My Drive/Research_Shanto/Datasets/Ashik Bhai_Sentiment/Code/real_imb_senti_test.csv',  format='CSV',\n",
        "    fields=[('text', TEXT), ('labels', LABEL)])\n",
        "\n",
        "# valid_data= data.TabularDataset(\n",
        "#     path='/content/drive/My Drive/Research_Shanto/Datasets/SemEvel/proc_SemEvel_2016_devtest.csv',  format='CSV',skip_header=True,\n",
        "#     fields=[('text', TEXT), ('labels', LABEL)])\n",
        "train_data, valid_data = train_data.split(split_ratio=0.85,random_state = random.seed(SEED))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rnb_ldOuqYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_VUS6mHQU_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_data.weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNuPs4b3QwfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(vars(valid_data.examples[6])['text'])\n",
        "print(tokens)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAJ8LdRDuqYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(vars(train_data.examples[6]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urV9k2iuuqYO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[6])['text'])\n",
        "\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwK1sgnguqYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JZpyBjeuqYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-WdiHBKB62r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(LABEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK0a2tLbuqYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "train_iterator,valid_iterator,test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data,test_data), \n",
        "    sort_key=lambda x: len(x.text),\n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deeacDO-uqYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "bert = BertModel.from_pretrained('bert-base-multilingual-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKF1EngTuqYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BERTGRUSentiment(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = bert\n",
        "        \n",
        "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        \n",
        "        self.rnn = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          num_layers = n_layers,\n",
        "                          bidirectional = bidirectional,\n",
        "                          batch_first = True,\n",
        "                          dropout = 0 if n_layers < 2 else dropout)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        #text = [batch size, sent len]\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            embedded = self.bert(text)[0]\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        _, hidden = self.rnn(embedded)\n",
        "\n",
        "\n",
        " \n",
        "        \n",
        "        #hidden = [n layers * n directions, batch size, emb dim]\n",
        "        \n",
        "        if self.rnn.bidirectional:\n",
        "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1,:,:])\n",
        "                \n",
        "        #hidden = [batch size, hid dim]\n",
        "        \n",
        "        output = self.out(hidden)\n",
        "\n",
        "\n",
        "        #output = [batch size, out dim]\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loinaxlFuqYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_DIM = 350\n",
        "OUTPUT_DIM = 3\n",
        "N_LAYERS = 1\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "\n",
        "model = BERTGRUSentiment(bert,\n",
        "                         HIDDEN_DIM,\n",
        "                         OUTPUT_DIM,\n",
        "                         N_LAYERS,\n",
        "                         BIDIRECTIONAL,\n",
        "                         DROPOUT)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We0Zo-zpuqYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T16Y1jHmuqYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5hX50KLuqYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pyxk1e_uuqYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-a2_rQiuqY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=5e-4, weight_decay=1e-6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIbZhRWDuqY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# weight = 1./ torch.FloatTensor([ , ])\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4mIImTnuqY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrCehk3xuqY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.max(preds, 1)[1]\n",
        "    # print(\"rounded_preds: \",rounded_preds)\n",
        "    # print(\"y: \",y)\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWiVAvIAuqY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    # hid = model.initialize_hidden_state(BATCH_SIZE, device)\n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "\n",
        "        predictions = model(batch.text)\n",
        "        \n",
        "        loss = criterion(predictions, batch.labels)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.labels)\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkOoMfX3uqZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    all_predictions = []\n",
        "    targs = []\n",
        "    # hid = model.initialize_hidden_state(BATCH_SIZE, device)\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            # hid= tuple([each.data for each in hid])\n",
        "\n",
        "            predictions = model(batch.text)\n",
        "            rounded_predictions = torch.max(predictions, 1)[1]\n",
        "            # print(\"rounded_predictions: \",rounded_predictions)\n",
        "            all_predictions.extend(rounded_predictions.cpu().detach().numpy())\n",
        "            # print(\"all_predictions: \",all_predictions)\n",
        "\n",
        "            targs.extend(batch.labels.cpu().detach().numpy())\n",
        "\n",
        "            loss = criterion(predictions, batch.labels)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.labels)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), all_predictions, targs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_GDmFqduqZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnLW2MJyuqZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_EPOCHS = 20\n",
        "loss_t = []\n",
        "loss_v = []\n",
        "y_ = []\n",
        "train_ac = []\n",
        "val_ac = []\n",
        "best_acc = float('inf')\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    y_.append(int(epoch+1))\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc ,a,b= evaluate(model, valid_iterator, criterion)\n",
        "        \n",
        "    end_time = time.time()\n",
        "        \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        best_acc = valid_acc\n",
        "        torch.save(model.state_dict(), '/content/drive/My Drive/Research_Shanto/Datasets/Ashik Bhai_Sentiment/Code/Gsuit_BertCNN.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "    loss_v.append(valid_loss)\n",
        "    loss_t.append(train_loss)\n",
        "    train_ac.append(train_acc)\n",
        "    val_ac.append(valid_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc6I0S8YuqZL",
        "colab_type": "text"
      },
      "source": [
        "We'll load up the parameters that gave us the best validation loss and try these on the test set - which gives us our best results so far!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLHoLA7VciFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_state_dict(torch.load('/content/drive/My Drive/Research_Shanto/Datasets/Ashik Bhai_Sentiment/Code/Gsuit_BertCNN.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b67Rz6PvuqZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc, all_predictions,targs = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2febgWmDIz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_sentiment(model, tokenizer, sentence_list):\n",
        "  final_ans = []\n",
        "  for s in sentence_list:\n",
        "    sentence = \"\"\n",
        "    sentence = sentence.join(s)\n",
        "    # print(sentence)\n",
        "    model.eval()\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    prediction = torch.round(model(tensor))\n",
        "    \n",
        "    # prediction = torch.round(prediction)\n",
        "    if(prediction[0][0] > prediction[0][1]):\n",
        "      final_ans.append('Negative')\n",
        "    else:\n",
        "      final_ans.append('Positive')\n",
        "  return final_ans\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdlbud1hDM71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/Research_Shanto/Datasets/aspect_dataset.csv')\n",
        "# data = data.astype(str).values.tolist()\n",
        "# print(data)\n",
        "data1 = data[data.aspect == 'religion']\n",
        "data1 = data1[data.polarity != 'nt']\n",
        "# print(data1.shape)\n",
        "print(data1.groupby('polarity').polarity.count())\n",
        "# data1 = data1.drop(['polarity'], axis = 1)\n",
        "data1 = data1.drop(['aspect'], axis = 1)\n",
        "data1 = data1.sample(n=334);\n",
        "print(data1.groupby('polarity').polarity.count())\n",
        "dataset = data1.post.astype(str).values.tolist()\n",
        "# print(data1.shape)\n",
        "\n",
        "print(data1.dtypes)\n",
        "print(data1.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Tv4-PNADYfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_ans = predict_sentiment(model, tokenizer, dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEDY9_fLDagv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(final_ans.count('Positive')/len(final_ans))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}